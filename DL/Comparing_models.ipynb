{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1Tb92zhXOKvMyUNzNwxO72FlRoupv9Yef",
      "authorship_tag": "ABX9TyPvl7kdSx3Sqo4I9nIEjRGB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7eb7024de67042b1a7a41d291bd5d6bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a69c03f867a14b8894dd0abbba71d2b8",
              "IPY_MODEL_f80fb83099624ea9a1d998153feda971",
              "IPY_MODEL_cbe4601a9d8847a5a81db8624ae2da88"
            ],
            "layout": "IPY_MODEL_fa218ce8d82849609eb92aac99833b8d"
          }
        },
        "a69c03f867a14b8894dd0abbba71d2b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2f14c29e982c45e9946cabc79d4bed9b",
            "placeholder": "​",
            "style": "IPY_MODEL_1bc3f86e509640f7a4ddf79d298c12fb",
            "value": "100%"
          }
        },
        "f80fb83099624ea9a1d998153feda971": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0b5172b8bde84bdda3e9b8164d218ccc",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5658d2111fa74e4da5b7c3e7d0d09d07",
            "value": 10
          }
        },
        "cbe4601a9d8847a5a81db8624ae2da88": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_13d5c080098145a9b781179298095531",
            "placeholder": "​",
            "style": "IPY_MODEL_6b73809ea5684010bd01142decc468a7",
            "value": " 10/10 [24:12&lt;00:00, 143.76s/epoch, loss=0.64, f1_score=[0.65710676], val_loss=0.632, val_f1_score=[0.6604317], lr=0.001]"
          }
        },
        "fa218ce8d82849609eb92aac99833b8d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2f14c29e982c45e9946cabc79d4bed9b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1bc3f86e509640f7a4ddf79d298c12fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0b5172b8bde84bdda3e9b8164d218ccc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5658d2111fa74e4da5b7c3e7d0d09d07": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "13d5c080098145a9b781179298095531": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6b73809ea5684010bd01142decc468a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ee7dc7b810c847c4bd666b9f813f1466": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6e02ca54a14646bf9b08ba9d32343fd1",
              "IPY_MODEL_b44c6b5c88d54380a97da789ab97c246",
              "IPY_MODEL_2268105bebb24a068a99e10a49ec7e51"
            ],
            "layout": "IPY_MODEL_752edf3660aa4b128667b763283daab8"
          }
        },
        "6e02ca54a14646bf9b08ba9d32343fd1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_323a39320a1e4aabb13e2ec607035255",
            "placeholder": "​",
            "style": "IPY_MODEL_ff5a993213804ed5967b3de079a9dab7",
            "value": "100%"
          }
        },
        "b44c6b5c88d54380a97da789ab97c246": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7ab063235f02428981dfa43167b9ca1f",
            "max": 59,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d5672b7cb6704208b12207608096fce9",
            "value": 59
          }
        },
        "2268105bebb24a068a99e10a49ec7e51": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f87cda93d97c4a9ab84247f0c17902c0",
            "placeholder": "​",
            "style": "IPY_MODEL_233492e141c0429993556972b432a2ea",
            "value": " 59.0/59.0 [02:13&lt;00:00, 1.91s/batch, loss=0.64, f1_score=[0.65710676]]"
          }
        },
        "752edf3660aa4b128667b763283daab8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "323a39320a1e4aabb13e2ec607035255": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ff5a993213804ed5967b3de079a9dab7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7ab063235f02428981dfa43167b9ca1f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d5672b7cb6704208b12207608096fce9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f87cda93d97c4a9ab84247f0c17902c0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "233492e141c0429993556972b432a2ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/spring-board-b2-hate-speech/Group-8/blob/madhuri/Comparing_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary packages\n",
        "#!pip install datasets pandas transformers tensorflow scikit-learn matplotlib\n",
        "\n",
        "import tensorflow as tf\n",
        "from transformers import TFBertModel, BertTokenizer\n",
        "#from datasets import load_dataset\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix, f1_score, precision_score, recall_score, accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Function to encode data using BERT tokenizer\n",
        "def encode_data(texts, tokenizer, max_len):\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "\n",
        "    for text in texts:\n",
        "        encoded = tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=max_len,\n",
        "            padding='max_length',\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='tf',\n",
        "            truncation=True\n",
        "        )\n",
        "        input_ids.append(encoded['input_ids'])\n",
        "        attention_masks.append(encoded['attention_mask'])\n",
        "\n",
        "    return tf.concat(input_ids, axis=0), tf.concat(attention_masks, axis=0)\n",
        "\n",
        "# Function to build BERT-based classification model\n",
        "def build_bert_model(max_len, bert_model_name='bert-base-uncased'):\n",
        "    # Load pre-trained BERT model and tokenizer\n",
        "    bert_model = TFBertModel.from_pretrained(bert_model_name)\n",
        "    tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
        "\n",
        "    # Define inputs\n",
        "    input_ids = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32, name=\"input_ids\")\n",
        "    attention_mask = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32, name=\"attention_mask\")\n",
        "\n",
        "    # BERT encoder\n",
        "    bert_output = bert_model(input_ids, attention_mask=attention_mask)\n",
        "    sequence_output = bert_output.last_hidden_state\n",
        "\n",
        "    # Adding custom layers on top of BERT\n",
        "    x = tf.keras.layers.Conv1D(filters=64, kernel_size=3, padding='same', activation='relu')(sequence_output)\n",
        "    x = tf.keras.layers.MaxPooling1D(pool_size=2)(x)\n",
        "    x = tf.keras.layers.Conv1D(filters=128, kernel_size=3, padding='same', activation='relu')(x)\n",
        "    x = tf.keras.layers.MaxPooling1D(pool_size=2)(x)\n",
        "    x = tf.keras.layers.Flatten()(x)\n",
        "    x = tf.keras.layers.Dense(256, activation='relu')(x)\n",
        "    x = tf.keras.layers.Dropout(0.5)(x)\n",
        "    output = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    # Define the model\n",
        "    model = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=output)\n",
        "\n",
        "    return model, tokenizer\n",
        "\n",
        "# Loading and preprocessing\n",
        "\n",
        "df = pd.read_csv(\"/content/Preprocessed_cleaned_Final_dataset.csv\")\n",
        "\n",
        "# Reduce dataset size for faster training\n",
        "df = df.sample(n=10)\n",
        "\n",
        "# Split the dataset into train and test sets\n",
        "train_df, test_df = train_test_split(df, test_size=0.2)\n",
        "\n",
        "max_len = 128\n",
        "batch_size = 32\n",
        "epochs = 1\n",
        "\n",
        "train_texts = train_df['Text'].tolist()  #Contains text data\n",
        "train_labels = train_df['IsHatespeech'].tolist()   # Contains labels\n",
        "test_texts = test_df['Text'].tolist()\n",
        "test_labels = test_df['IsHatespeech'].tolist()\n",
        "\n",
        "#string labels to numerical labels (assuming binary classification)\n",
        "train_labels = [0 if label == '0' else 1 for label in train_labels]\n",
        "test_labels = [0 if label == '0' else 1 for label in test_labels]\n",
        "\n",
        "model, tokenizer = build_bert_model(max_len)\n",
        "\n",
        "train_input_ids, train_attention_masks = encode_data(train_texts, tokenizer, max_len)\n",
        "test_input_ids, test_attention_masks = encode_data(test_texts, tokenizer, max_len)\n",
        "\n",
        "train_labels = tf.convert_to_tensor(train_labels)\n",
        "test_labels = tf.convert_to_tensor(test_labels)\n",
        "\n",
        "# Compiling\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Model Training\n",
        "history = model.fit(\n",
        "    [train_input_ids, train_attention_masks], train_labels,\n",
        "    validation_data=([test_input_ids, test_attention_masks], test_labels),\n",
        "    epochs=epochs,\n",
        "    batch_size=batch_size\n",
        ")\n",
        "\n",
        "# Model eval\n",
        "results = model.evaluate([test_input_ids, test_attention_masks], test_labels)\n",
        "\n",
        "print(results)\n",
        "\n",
        "# Predictions for classification report\n",
        "test_predictions = model.predict([test_input_ids, test_attention_masks])\n",
        "test_predictions = np.round(test_predictions).astype(int).flatten()\n",
        "\n",
        "# Classification report\n",
        "print(\"Deep Learning Model Performance:\")\n",
        "print(\"Classification Report:\\n\", classification_report(test_labels, test_predictions))\n",
        "print(\"Accuracy:\", accuracy_score(test_labels, test_predictions))\n",
        "print(\"AUC-ROC:\", roc_auc_score(test_labels, test_predictions))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(test_labels, test_predictions))\n",
        "print(\"F1 Score:\", f1_score(test_labels, test_predictions))\n",
        "print(\"Precision:\", precision_score(test_labels, test_predictions))\n",
        "print(\"Recall:\", recall_score(test_labels, test_predictions))\n",
        "\n",
        "#training history\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend(loc='upper left')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(loc='upper left')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 810
        },
        "id": "7DB8ULuDD9Wv",
        "outputId": "d78c6e26-2ccb-4dd8-a9ab-ccf9445eea9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFBertModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_3/bert/pooler/dense/kernel:0', 'tf_bert_model_3/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_3/bert/pooler/dense/kernel:0', 'tf_bert_model_3/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_3/bert/pooler/dense/kernel:0', 'tf_bert_model_3/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_3/bert/pooler/dense/kernel:0', 'tf_bert_model_3/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 168s 168s/step - loss: 0.6860 - accuracy: 0.5000 - val_loss: 0.6564 - val_accuracy: 1.0000\n",
            "1/1 [==============================] - 1s 821ms/step - loss: 0.6564 - accuracy: 1.0000\n",
            "[0.6563659906387329, 1.0]\n",
            "1/1 [==============================] - 13s 13s/step\n",
            "Deep Learning Model Performance:\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           1       1.00      1.00      1.00         2\n",
            "\n",
            "    accuracy                           1.00         2\n",
            "   macro avg       1.00      1.00      1.00         2\n",
            "weighted avg       1.00      1.00      1.00         2\n",
            "\n",
            "Accuracy: 1.0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Only one class present in y_true. ROC AUC score is not defined in that case.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-17cba952739e>\u001b[0m in \u001b[0;36m<cell line: 119>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Classification Report:\\n\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_predictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Accuracy:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_predictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"AUC-ROC:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_predictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Confusion Matrix:\\n\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_predictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"F1 Score:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_predictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py\u001b[0m in \u001b[0;36mroc_auc_score\u001b[0;34m(y_true, y_score, average, sample_weight, max_fpr, multi_class, labels)\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel_binarize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m         return _average_binary_score(\n\u001b[0m\u001b[1;32m    573\u001b[0m             \u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_binary_roc_auc_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_fpr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_fpr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m             \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_base.py\u001b[0m in \u001b[0;36m_average_binary_score\u001b[0;34m(binary_metric, y_true, y_score, average, sample_weight)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbinary_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_ranking.py\u001b[0m in \u001b[0;36m_binary_roc_auc_score\u001b[0;34m(y_true, y_score, sample_weight, max_fpr)\u001b[0m\n\u001b[1;32m    337\u001b[0m     \u001b[0;34m\"\"\"Binary roc auc score.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    340\u001b[0m             \u001b[0;34m\"Only one class present in y_true. ROC AUC score \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m             \u001b[0;34m\"is not defined in that case.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Only one class present in y_true. ROC AUC score is not defined in that case."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, LSTM, Dense, Dropout, Flatten\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "# Load dataset\n",
        "train_df = pd.read_csv('Train_data.csv')\n",
        "test_df = pd.read_csv('Test_data.csv')\n",
        "\n",
        "# Combine text data for tokenization and word2vec training\n",
        "texts = pd.concat([train_df['clean_text'], test_df['clean_text']]).tolist()\n",
        "\n",
        "# Tokenize the text data\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences_train = tokenizer.texts_to_sequences(train_df['clean_text'])\n",
        "sequences_test = tokenizer.texts_to_sequences(test_df['clean_text'])\n",
        "\n",
        "# Get vocabulary size\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "# Pad sequences\n",
        "max_seq_length = max(max([len(seq) for seq in sequences_train]), max([len(seq) for seq in sequences_test]))\n",
        "X_train_pad = pad_sequences(sequences_train, maxlen=max_seq_length)\n",
        "X_test_pad = pad_sequences(sequences_test, maxlen=max_seq_length)\n",
        "\n",
        "# Train Word2Vec model\n",
        "sentences = [text.split() for text in texts]\n",
        "word2vec_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Create embedding matrix\n",
        "embedding_matrix = np.zeros((vocab_size, 100))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    if word in word2vec_model.wv:\n",
        "        embedding_matrix[i] = word2vec_model.wv[word]\n",
        "\n",
        "# Prepare labels\n",
        "y_train = train_df['IsHatespeech'].values\n",
        "y_test = test_df['IsHatespeech'].values\n",
        "\n",
        "# Build the model\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=vocab_size, output_dim=100, weights=[embedding_matrix], input_length=max_seq_length, trainable=False))\n",
        "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
        "\n",
        "# Callbacks\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=0.0001)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train_pad, y_train, epochs=5, batch_size=128, validation_data=(X_test_pad, y_test), verbose=2, callbacks=[early_stopping, reduce_lr])\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test_pad, y_test, verbose=0)\n",
        "print(f\"Test Accuracy: {accuracy}\")\n",
        "\n",
        "# Predict and evaluate performance\n",
        "y_pred_prob = model.predict(X_test_pad)\n",
        "y_pred = (y_pred_prob > 0.5).astype(\"int32\").flatten()\n",
        "\n",
        "from sklearn.metrics import classification_report, accuracy_score, roc_auc_score, confusion_matrix, f1_score, precision_score, recall_score\n",
        "\n",
        "print(\"Deep Learning Model Performance with Word2Vec:\")\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"AUC-ROC:\", roc_auc_score(y_test, y_pred_prob))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
        "print(\"F1 Score:\", f1_score(y_test, y_pred))\n",
        "print(\"Precision:\", precision_score(y_test, y_pred))\n",
        "print(\"Recall:\", recall_score(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7DI7S3pU4Cd",
        "outputId": "daaf497a-971e-4f9e-b5b2-db107fef4ef0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "59/59 - 95s - loss: 0.6255 - accuracy: 0.6469 - val_loss: 0.6206 - val_accuracy: 0.6622 - lr: 0.0010 - 95s/epoch - 2s/step\n",
            "Epoch 2/5\n",
            "59/59 - 102s - loss: 0.5949 - accuracy: 0.6760 - val_loss: 0.6264 - val_accuracy: 0.6369 - lr: 0.0010 - 102s/epoch - 2s/step\n",
            "Epoch 3/5\n",
            "59/59 - 115s - loss: 0.6113 - accuracy: 0.6466 - val_loss: 0.5895 - val_accuracy: 0.6799 - lr: 0.0010 - 115s/epoch - 2s/step\n",
            "Epoch 4/5\n",
            "59/59 - 82s - loss: 0.5803 - accuracy: 0.6869 - val_loss: 0.5827 - val_accuracy: 0.6880 - lr: 0.0010 - 82s/epoch - 1s/step\n",
            "Epoch 5/5\n",
            "59/59 - 81s - loss: 0.5795 - accuracy: 0.6923 - val_loss: 0.5990 - val_accuracy: 0.6729 - lr: 0.0010 - 81s/epoch - 1s/step\n",
            "Test Accuracy: 0.6729323267936707\n",
            "59/59 [==============================] - 4s 64ms/step\n",
            "Deep Learning Model Performance with Word2Vec:\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.68      0.66      0.67       944\n",
            "           1       0.66      0.69      0.67       918\n",
            "\n",
            "    accuracy                           0.67      1862\n",
            "   macro avg       0.67      0.67      0.67      1862\n",
            "weighted avg       0.67      0.67      0.67      1862\n",
            "\n",
            "Accuracy: 0.6729323308270677\n",
            "AUC-ROC: 0.7453657545880875\n",
            "Confusion Matrix:\n",
            " [[622 322]\n",
            " [287 631]]\n",
            "F1 Score: 0.6745056119722074\n",
            "Precision: 0.6621196222455404\n",
            "Recall: 0.6873638344226579\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, LSTM, Dense, Dropout, Flatten\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "# Load dataset\n",
        "train_df = pd.read_csv('Train_data.csv')\n",
        "test_df = pd.read_csv('Test_data.csv')\n",
        "\n",
        "# Combine text data for tokenization and word2vec training\n",
        "texts = pd.concat([train_df['clean_text'], test_df['clean_text']]).tolist()\n",
        "\n",
        "# Tokenize the text data\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences_train = tokenizer.texts_to_sequences(train_df['clean_text'])\n",
        "sequences_test = tokenizer.texts_to_sequences(test_df['clean_text'])\n",
        "\n",
        "# Get vocabulary size\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "# Pad sequences\n",
        "max_seq_length = max(max([len(seq) for seq in sequences_train]), max([len(seq) for seq in sequences_test]))\n",
        "X_train_pad = pad_sequences(sequences_train, maxlen=max_seq_length)\n",
        "X_test_pad = pad_sequences(sequences_test, maxlen=max_seq_length)\n",
        "\n",
        "# Train Word2Vec model\n",
        "sentences = [text.split() for text in texts]\n",
        "word2vec_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Create embedding matrix\n",
        "embedding_matrix = np.zeros((vocab_size, 100))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    if word in word2vec_model.wv:\n",
        "        embedding_matrix[i] = word2vec_model.wv[word]\n",
        "\n",
        "# Prepare labels\n",
        "y_train = train_df['IsHatespeech'].values\n",
        "y_test = test_df['IsHatespeech'].values\n",
        "\n",
        "# Build the model with more neurons and additional Dense layers\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=vocab_size, output_dim=100, weights=[embedding_matrix], input_length=max_seq_length, trainable=False))\n",
        "model.add(LSTM(256, dropout=0.3, recurrent_dropout=0.3))  # Increased neurons and dropout\n",
        "model.add(Dense(128, activation='relu'))  # Increased neurons\n",
        "model.add(Dropout(0.3))  # Increased dropout\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
        "\n",
        "# Callbacks\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=0.0001)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train_pad, y_train, epochs=1, batch_size=128, validation_data=(X_test_pad, y_test), verbose=2, callbacks=[early_stopping, reduce_lr])\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test_pad, y_test, verbose=0)\n",
        "print(f\"Test Accuracy: {accuracy}\")\n",
        "\n",
        "# Predict and evaluate performance\n",
        "y_pred_prob = model.predict(X_test_pad)\n",
        "y_pred = (y_pred_prob > 0.5).astype(\"int32\").flatten()\n",
        "\n",
        "from sklearn.metrics import classification_report, accuracy_score, roc_auc_score, confusion_matrix, f1_score, precision_score, recall_score\n",
        "\n",
        "print(\"Deep Learning Model Performance with Word2Vec:\")\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"AUC-ROC:\", roc_auc_score(y_test, y_pred_prob))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
        "print(\"F1 Score:\", f1_score(y_test, y_pred))\n",
        "print(\"Precision:\", precision_score(y_test, y_pred))\n",
        "print(\"Recall:\", recall_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fSfUrdWYWaiu",
        "outputId": "a7643fdf-fd01-4182-b76d-250fbbe4e57e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "59/59 - 163s - loss: 0.6284 - accuracy: 0.6351 - val_loss: 0.6064 - val_accuracy: 0.6762 - lr: 0.0010 - 163s/epoch - 3s/step\n",
            "Test Accuracy: 0.6761546730995178\n",
            "59/59 [==============================] - 17s 284ms/step\n",
            "Deep Learning Model Performance with Word2Vec:\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.53      0.63       944\n",
            "           1       0.63      0.82      0.71       918\n",
            "\n",
            "    accuracy                           0.68      1862\n",
            "   macro avg       0.69      0.68      0.67      1862\n",
            "weighted avg       0.69      0.68      0.67      1862\n",
            "\n",
            "Accuracy: 0.6761546723952739\n",
            "AUC-ROC: 0.7365011447140061\n",
            "Confusion Matrix:\n",
            " [[504 440]\n",
            " [163 755]]\n",
            "F1 Score: 0.7146237576904875\n",
            "Precision: 0.6317991631799164\n",
            "Recall: 0.8224400871459695\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, LSTM, Dense, Dropout, Flatten\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "# Load dataset\n",
        "train_df = pd.read_csv('Train_data.csv')\n",
        "test_df = pd.read_csv('Test_data.csv')\n",
        "\n",
        "# Combine text data for tokenization and word2vec training\n",
        "texts = pd.concat([train_df['clean_text'], test_df['clean_text']]).tolist()\n",
        "\n",
        "# Tokenize the text data\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences_train = tokenizer.texts_to_sequences(train_df['clean_text'])\n",
        "sequences_test = tokenizer.texts_to_sequences(test_df['clean_text'])\n",
        "\n",
        "# Get vocabulary size\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "# Pad sequences\n",
        "max_seq_length = max(max([len(seq) for seq in sequences_train]), max([len(seq) for seq in sequences_test]))\n",
        "X_train_pad = pad_sequences(sequences_train, maxlen=max_seq_length)\n",
        "X_test_pad = pad_sequences(sequences_test, maxlen=max_seq_length)\n",
        "\n",
        "# Train Word2Vec model\n",
        "sentences = [text.split() for text in texts]\n",
        "word2vec_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Create embedding matrix\n",
        "embedding_matrix = np.zeros((vocab_size, 100))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    if word in word2vec_model.wv:\n",
        "        embedding_matrix[i] = word2vec_model.wv[word]\n",
        "\n",
        "# Prepare labels\n",
        "y_train = train_df['IsHatespeech'].values\n",
        "y_test = test_df['IsHatespeech'].values\n",
        "\n",
        "# Split the data\n",
        "X = train_df['clean_text']\n",
        "y = train_df['IsHatespeech']\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
        "\n",
        "# Tokenize and pad sequences for the training and validation data\n",
        "sequences_train = tokenizer.texts_to_sequences(X_train)\n",
        "sequences_val = tokenizer.texts_to_sequences(X_val)\n",
        "\n",
        "X_train_pad = pad_sequences(sequences_train, maxlen=max_seq_length)\n",
        "X_val_pad = pad_sequences(sequences_val, maxlen=max_seq_length)\n",
        "\n",
        "# Resample (oversample) the training data\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_res, y_train_res = smote.fit_resample(X_train_pad, y_train)\n",
        "\n",
        "# Build the model with more neurons and additional Dense layers\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=vocab_size, output_dim=100, weights=[embedding_matrix], input_length=max_seq_length, trainable=False))\n",
        "model.add(LSTM(256, dropout=0.3, recurrent_dropout=0.3))  # Increased neurons and dropout\n",
        "model.add(Dense(128, activation='relu'))  # Increased neurons\n",
        "model.add(Dropout(0.3))  # Increased dropout\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
        "\n",
        "# Callbacks\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=0.0001)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train_res, y_train_res, epochs=5, batch_size=128, validation_data=(X_val_pad, y_val), verbose=2, callbacks=[early_stopping, reduce_lr])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NI77KEFUQ-dT",
        "outputId": "df06528c-473f-404c-bba6-8a19736f3a35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "48/48 - 217s - loss: 0.6570 - accuracy: 0.5820 - val_loss: 0.6065 - val_accuracy: 0.6582 - lr: 0.0010 - 217s/epoch - 5s/step\n",
            "Epoch 2/5\n",
            "48/48 - 137s - loss: 0.6214 - accuracy: 0.6614 - val_loss: 0.6285 - val_accuracy: 0.6286 - lr: 0.0010 - 137s/epoch - 3s/step\n",
            "Epoch 3/5\n",
            "48/48 - 130s - loss: 0.6154 - accuracy: 0.6624 - val_loss: 0.6065 - val_accuracy: 0.6521 - lr: 0.0010 - 130s/epoch - 3s/step\n",
            "Epoch 4/5\n",
            "48/48 - 130s - loss: 0.5939 - accuracy: 0.6800 - val_loss: 0.5629 - val_accuracy: 0.7045 - lr: 2.0000e-04 - 130s/epoch - 3s/step\n",
            "Epoch 5/5\n",
            "48/48 - 146s - loss: 0.5924 - accuracy: 0.6875 - val_loss: 0.5638 - val_accuracy: 0.7119 - lr: 2.0000e-04 - 146s/epoch - 3s/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss, accuracy = model.evaluate(X_test_pad, y_test, verbose=0)\n",
        "print(f\"Test Accuracy: {accuracy}\")\n",
        "\n",
        "\n",
        "y_pred_prob = model.predict(X_test_pad)\n",
        "y_pred = (y_pred_prob > 0.5).astype(\"int32\").flatten()\n",
        "\n",
        "from sklearn.metrics import classification_report, accuracy_score, roc_auc_score, confusion_matrix, f1_score, precision_score, recall_score\n",
        "\n",
        "print(\"Deep Learning Model Performance with Word2Vec:\")\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"AUC-ROC:\", roc_auc_score(y_test, y_pred_prob))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
        "print(\"F1 Score:\", f1_score(y_test, y_pred))\n",
        "print(\"Precision:\", precision_score(y_test, y_pred))\n",
        "print(\"Recall:\", recall_score(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZH5-p8PxVfYa",
        "outputId": "d5b69967-9308-4a9c-891f-83373af947e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.6769067645072937\n",
            "59/59 [==============================] - 19s 309ms/step\n",
            "Deep Learning Model Performance with Word2Vec:\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.55      0.63       944\n",
            "           1       0.64      0.81      0.71       918\n",
            "\n",
            "    accuracy                           0.68      1862\n",
            "   macro avg       0.69      0.68      0.67      1862\n",
            "weighted avg       0.70      0.68      0.67      1862\n",
            "\n",
            "Accuracy: 0.6793770139634802\n",
            "AUC-ROC: 0.7450380340460102\n",
            "Confusion Matrix:\n",
            " [[518 426]\n",
            " [171 747]]\n",
            "F1 Score: 0.7144906743185079\n",
            "Precision: 0.6368286445012787\n",
            "Recall: 0.8137254901960784\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install tensorflow\n",
        "import tensorflow as tf # Import tensorflow\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tqdm.keras import TqdmCallback\n",
        "\n",
        "# Load dataset\n",
        "train_df = pd.read_csv('Train_data.csv')\n",
        "test_df = pd.read_csv('Test_data.csv')\n",
        "\n",
        "# Combine\n",
        "texts = pd.concat([train_df['clean_text'], test_df['clean_text']]).tolist()\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences_train = tokenizer.texts_to_sequences(train_df['clean_text'])\n",
        "sequences_test = tokenizer.texts_to_sequences(test_df['clean_text'])\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "max_seq_length = max(max([len(seq) for seq in sequences_train]), max([len(seq) for seq in sequences_test]))\n",
        "X_train_pad = pad_sequences(sequences_train, maxlen=max_seq_length)\n",
        "X_test_pad = pad_sequences(sequences_test, maxlen=max_seq_length)\n",
        "\n",
        "sentences = [text.split() for text in texts]\n",
        "word2vec_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "embedding_matrix = np.zeros((vocab_size, 100))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    if word in word2vec_model.wv:\n",
        "        embedding_matrix[i] = word2vec_model.wv[word]\n",
        "\n",
        "y_train = train_df['IsHatespeech'].values\n",
        "y_test = test_df['IsHatespeech'].values\n",
        "\n",
        "y_train = y_train.astype(np.float32)  # Cast to float32\n",
        "y_test = y_test.astype(np.float32)  # Cast to float32\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=vocab_size, output_dim=100, weights=[embedding_matrix], input_length=max_seq_length, trainable=False))\n",
        "model.add(Bidirectional(LSTM(1024, return_sequences=True)))\n",
        "model.add(Bidirectional(LSTM(512, return_sequences=True)))\n",
        "model.add(Bidirectional(LSTM(256, dropout=0.3, recurrent_dropout=0.3)))\n",
        "model.add(Dense(128, activation='sigmoid'))\n",
        "model.add(Dropout(0.4))\n",
        "model.add(Dense(64, activation='sigmoid'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=[tf.keras.metrics.F1Score(dtype='float32')])\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=0.0001)\n",
        "\n",
        "# Fit the model with tqdm progress bar\n",
        "history = model.fit(X_train_pad, y_train, epochs=10, batch_size=128, validation_data=(X_test_pad, y_test), verbose=0, callbacks=[early_stopping, reduce_lr, TqdmCallback(verbose=1)])\n",
        "\n",
        "loss, accuracy = model.evaluate(X_test_pad, y_test, verbose=0)\n",
        "print(f\"Test Accuracy: {accuracy}\")\n",
        "\n",
        "y_pred_prob = model.predict(X_test_pad)\n",
        "y_pred = (y_pred_prob > 0.5).astype(\"int32\").flatten()\n",
        "\n",
        "from sklearn.metrics import classification_report, accuracy_score, roc_auc_score, confusion_matrix, f1_score, precision_score, recall_score\n",
        "\n",
        "print(\"Deep Learning Model Performance with Word2Vec:\")\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"AUC-ROC:\", roc_auc_score(y_test, y_pred_prob))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
        "print(\"F1 Score:\", f1_score(y_test, y_pred))\n",
        "print(\"Precision:\", precision_score(y_test, y_pred))\n",
        "print(\"Recall:\", recall_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 512,
          "referenced_widgets": [
            "7eb7024de67042b1a7a41d291bd5d6bf",
            "a69c03f867a14b8894dd0abbba71d2b8",
            "f80fb83099624ea9a1d998153feda971",
            "cbe4601a9d8847a5a81db8624ae2da88",
            "fa218ce8d82849609eb92aac99833b8d",
            "2f14c29e982c45e9946cabc79d4bed9b",
            "1bc3f86e509640f7a4ddf79d298c12fb",
            "0b5172b8bde84bdda3e9b8164d218ccc",
            "5658d2111fa74e4da5b7c3e7d0d09d07",
            "13d5c080098145a9b781179298095531",
            "6b73809ea5684010bd01142decc468a7",
            "ee7dc7b810c847c4bd666b9f813f1466",
            "6e02ca54a14646bf9b08ba9d32343fd1",
            "b44c6b5c88d54380a97da789ab97c246",
            "2268105bebb24a068a99e10a49ec7e51",
            "752edf3660aa4b128667b763283daab8",
            "323a39320a1e4aabb13e2ec607035255",
            "ff5a993213804ed5967b3de079a9dab7",
            "7ab063235f02428981dfa43167b9ca1f",
            "d5672b7cb6704208b12207608096fce9",
            "f87cda93d97c4a9ab84247f0c17902c0",
            "233492e141c0429993556972b432a2ea"
          ]
        },
        "id": "7yB617vtg8OC",
        "outputId": "48126a7a-124c-4cf2-927c-b562eaca76a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_8 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_8 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_8 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0epoch [00:00, ?epoch/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7eb7024de67042b1a7a41d291bd5d6bf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0batch [00:00, ?batch/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ee7dc7b810c847c4bd666b9f813f1466"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: [0.6604317]\n",
            "59/59 [==============================] - 14s 219ms/step\n",
            "Deep Learning Model Performance with Word2Vec:\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.66      0.67      0.66       944\n",
            "         1.0       0.66      0.65      0.65       918\n",
            "\n",
            "    accuracy                           0.66      1862\n",
            "   macro avg       0.66      0.66      0.66      1862\n",
            "weighted avg       0.66      0.66      0.66      1862\n",
            "\n",
            "Accuracy: 0.658968850698174\n",
            "AUC-ROC: 0.7164346082123999\n",
            "Confusion Matrix:\n",
            " [[630 314]\n",
            " [321 597]]\n",
            "F1 Score: 0.6528157463094588\n",
            "Precision: 0.6553238199780461\n",
            "Recall: 0.6503267973856209\n"
          ]
        }
      ]
    }
  ]
}