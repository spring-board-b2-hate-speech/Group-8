{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e9c64f5",
   "metadata": {},
   "source": [
    "## Importing the Traing and Testing split dataset and Performing TF-IDF Transformation on them to use for the ML Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4e3fe415",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "train_df = pd.read_csv('Train_data.csv')\n",
    "test_df = pd.read_csv('Test_data.csv')\n",
    "\n",
    "X_train = train_df['clean_text']\n",
    "y_train = train_df['IsHatespeech']\n",
    "X_test = test_df['clean_text']\n",
    "y_test = test_df['IsHatespeech']\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=2500)\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526b2997",
   "metadata": {},
   "source": [
    "### Support Vector Model\n",
    "#### This model was used to set a standard benchmark score for the dataset. Here the model was Hypertunned and best fit parameters were tasted and the parameter which seemed best were used to train the model. We considered the Accuracy and F1 score to evaluate the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0133efc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Performance with TF-IDF:\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.73      0.76       944\n",
      "           1       0.74      0.80      0.77       918\n",
      "\n",
      "    accuracy                           0.76      1862\n",
      "   macro avg       0.77      0.76      0.76      1862\n",
      "weighted avg       0.77      0.76      0.76      1862\n",
      "\n",
      "Accuracy: 0.7636949516648764\n",
      "AUC-ROC: 0.8351375272331156\n",
      "Confusion Matrix:\n",
      " [[686 258]\n",
      " [182 736]]\n",
      "F1 Score: 0.7698744769874477\n",
      "Precision: 0.7404426559356136\n",
      "Recall: 0.8017429193899782\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score, roc_auc_score, precision_score, recall_score, f1_score\n",
    "\n",
    "svm_model = SVC(C=1, gamma='scale', kernel='rbf', random_state=42, probability=True)\n",
    "\n",
    "svm_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "y_pred_svm = svm_model.predict(X_test_tfidf)\n",
    "y_pred_svm_proba = svm_model.predict_proba(X_test_tfidf)[:, 1]\n",
    "\n",
    "print(\"SVM Performance with TF-IDF:\")\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_svm))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_svm))\n",
    "print(\"AUC-ROC:\", roc_auc_score(y_test, y_pred_svm_proba))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_svm))\n",
    "print(\"F1 Score:\", f1_score(y_test, y_pred_svm))\n",
    "print(\"Precision:\",precision_score(y_test, y_pred_svm))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred_svm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec781f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
