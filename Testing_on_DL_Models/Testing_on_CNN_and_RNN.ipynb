{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WVZobaRNJvZJ",
        "outputId": "4d78b9e1-8db0-4ee5-a1a3-ba95d97b9202"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-07-11 14:18:10--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2024-07-11 14:18:10--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2024-07-11 14:18:10--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.02MB/s    in 2m 39s  \n",
            "\n",
            "2024-07-11 14:20:49 (5.18 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!unzip glove.6B.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A2I3eoaGKJ1z",
        "outputId": "bc553d4d-31bc-49e1-dd67-65be3de7055b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Conv1D, GlobalMaxPooling1D, Embedding, Input\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Load the dataset\n",
        "train_df = pd.read_csv('Train_data.csv')\n",
        "test_df = pd.read_csv('Test_data.csv')\n",
        "\n",
        "# Split into features and labels\n",
        "X_train = train_df['clean_text']\n",
        "y_train = train_df['IsHatespeech']\n",
        "X_test = test_df['clean_text']\n",
        "y_test = test_df['IsHatespeech']\n",
        "\n",
        "# Tokenize the text\n",
        "max_words = 2500\n",
        "max_len = 128\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
        "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "X_train_pad = pad_sequences(X_train_seq, maxlen=max_len)\n",
        "X_test_pad = pad_sequences(X_test_seq, maxlen=max_len)\n",
        "\n",
        "# Load GloVe embeddings\n",
        "embedding_dim = 100\n",
        "embedding_index = {}\n",
        "\n",
        "glove_file = 'glove.6B.100d.txt'\n",
        "\n",
        "with open(glove_file, encoding=\"utf8\") as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embedding_index[word] = coefs\n",
        "\n",
        "# Prepare embedding matrix\n",
        "word_index = tokenizer.word_index\n",
        "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
        "\n",
        "for word, i in word_index.items():\n",
        "    if i < max_words:\n",
        "        embedding_vector = embedding_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "\n",
        "# Define the CNN model with GloVe embeddings\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_words, embedding_dim, input_length=max_len))\n",
        "model.add(Conv1D(128, 5, activation='relu'))\n",
        "model.add(GlobalMaxPooling1D())\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Load pre-trained GloVe embeddings into the Embedding layer\n",
        "model.layers[0].set_weights([embedding_matrix])\n",
        "model.layers[0].trainable = False\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train_pad, y_train, epochs=10, batch_size=32, validation_data=(X_test_pad, y_test))\n",
        "\n",
        "# Predict on test data\n",
        "y_pred_prob = model.predict(X_test_pad)\n",
        "y_pred = (y_pred_prob > 0.5).astype(int)  # Convert probabilities to binary predictions\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1-score:\", f1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gRalMbU4K8Sq",
        "outputId": "d7db324f-2e51-473d-8fb0-25c23f83c3a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "233/233 [==============================] - 16s 61ms/step - loss: 0.5826 - accuracy: 0.6900 - val_loss: 0.5530 - val_accuracy: 0.7186\n",
            "Epoch 2/10\n",
            "233/233 [==============================] - 14s 59ms/step - loss: 0.4843 - accuracy: 0.7605 - val_loss: 0.5018 - val_accuracy: 0.7599\n",
            "Epoch 3/10\n",
            "233/233 [==============================] - 8s 32ms/step - loss: 0.4160 - accuracy: 0.8067 - val_loss: 0.5156 - val_accuracy: 0.7556\n",
            "Epoch 4/10\n",
            "233/233 [==============================] - 15s 63ms/step - loss: 0.3447 - accuracy: 0.8428 - val_loss: 0.5505 - val_accuracy: 0.7438\n",
            "Epoch 5/10\n",
            "233/233 [==============================] - 12s 50ms/step - loss: 0.2617 - accuracy: 0.8881 - val_loss: 0.5931 - val_accuracy: 0.7390\n",
            "Epoch 6/10\n",
            "233/233 [==============================] - 8s 36ms/step - loss: 0.2008 - accuracy: 0.9163 - val_loss: 0.6965 - val_accuracy: 0.7379\n",
            "Epoch 7/10\n",
            "233/233 [==============================] - 14s 60ms/step - loss: 0.1496 - accuracy: 0.9409 - val_loss: 0.8119 - val_accuracy: 0.7320\n",
            "Epoch 8/10\n",
            "233/233 [==============================] - 10s 44ms/step - loss: 0.1253 - accuracy: 0.9499 - val_loss: 0.9111 - val_accuracy: 0.7154\n",
            "Epoch 9/10\n",
            "233/233 [==============================] - 9s 37ms/step - loss: 0.0922 - accuracy: 0.9659 - val_loss: 1.0237 - val_accuracy: 0.7240\n",
            "Epoch 10/10\n",
            "233/233 [==============================] - 14s 60ms/step - loss: 0.0850 - accuracy: 0.9664 - val_loss: 1.2790 - val_accuracy: 0.7154\n",
            "59/59 [==============================] - 1s 21ms/step\n",
            "Accuracy: 0.715359828141783\n",
            "Precision: 0.6632996632996633\n",
            "Recall: 0.8583877995642701\n",
            "F1-score: 0.748338081671415\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Conv1D, GlobalMaxPooling1D, Embedding, Input, Bidirectional, LSTM\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Load the dataset\n",
        "train_df = pd.read_csv('Train_data.csv')\n",
        "test_df = pd.read_csv('Test_data.csv')\n",
        "\n",
        "# Split into features and labels\n",
        "X_train = train_df['clean_text']\n",
        "y_train = train_df['IsHatespeech']\n",
        "X_test = test_df['clean_text']\n",
        "y_test = test_df['IsHatespeech']\n",
        "\n",
        "# Tokenize the text\n",
        "max_words = 10000  # Increase the max words to capture more vocabulary\n",
        "max_len = 128\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
        "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "X_train_pad = pad_sequences(X_train_seq, maxlen=max_len)\n",
        "X_test_pad = pad_sequences(X_test_seq, maxlen=max_len)\n",
        "\n",
        "# Load GloVe embeddings\n",
        "embedding_dim = 100\n",
        "embedding_index = {}\n",
        "\n",
        "glove_file = 'glove.6B.100d.txt'\n",
        "\n",
        "with open(glove_file, encoding=\"utf8\") as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embedding_index[word] = coefs\n",
        "\n",
        "# Prepare embedding matrix\n",
        "word_index = tokenizer.word_index\n",
        "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
        "\n",
        "for word, i in word_index.items():\n",
        "    if i < max_words:\n",
        "        embedding_vector = embedding_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "\n",
        "# Define the CNN model with GloVe embeddings\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_words, embedding_dim, input_length=max_len, weights=[embedding_matrix], trainable=True))\n",
        "model.add(Conv1D(128, 5, activation='relu'))\n",
        "model.add(Conv1D(128, 5, activation='relu'))\n",
        "model.add(GlobalMaxPooling1D())\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train_pad, y_train, epochs=10, batch_size=32, validation_data=(X_test_pad, y_test))\n",
        "\n",
        "# Predict on test data\n",
        "y_pred_prob = model.predict(X_test_pad)\n",
        "y_pred = (y_pred_prob > 0.5).astype(int)  # Convert probabilities to binary predictions\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1-score:\", f1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53tjFLy8LJbi",
        "outputId": "19f26eb7-a669-4dba-ddec-0f54e410d219"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "233/233 [==============================] - 36s 147ms/step - loss: 0.5731 - accuracy: 0.6884 - val_loss: 0.5601 - val_accuracy: 0.7127\n",
            "Epoch 2/10\n",
            "233/233 [==============================] - 33s 143ms/step - loss: 0.4328 - accuracy: 0.8008 - val_loss: 0.4926 - val_accuracy: 0.7648\n",
            "Epoch 3/10\n",
            "233/233 [==============================] - 34s 147ms/step - loss: 0.2984 - accuracy: 0.8772 - val_loss: 0.5383 - val_accuracy: 0.7621\n",
            "Epoch 4/10\n",
            "233/233 [==============================] - 40s 173ms/step - loss: 0.1699 - accuracy: 0.9357 - val_loss: 0.7064 - val_accuracy: 0.7465\n",
            "Epoch 5/10\n",
            "233/233 [==============================] - 37s 159ms/step - loss: 0.0919 - accuracy: 0.9688 - val_loss: 0.8787 - val_accuracy: 0.7535\n",
            "Epoch 6/10\n",
            "233/233 [==============================] - 38s 164ms/step - loss: 0.0551 - accuracy: 0.9803 - val_loss: 1.1541 - val_accuracy: 0.7513\n",
            "Epoch 7/10\n",
            "233/233 [==============================] - 40s 172ms/step - loss: 0.0509 - accuracy: 0.9824 - val_loss: 1.2786 - val_accuracy: 0.7508\n",
            "Epoch 8/10\n",
            "233/233 [==============================] - 41s 174ms/step - loss: 0.0304 - accuracy: 0.9902 - val_loss: 1.3899 - val_accuracy: 0.7454\n",
            "Epoch 9/10\n",
            "233/233 [==============================] - 39s 167ms/step - loss: 0.0211 - accuracy: 0.9923 - val_loss: 1.6470 - val_accuracy: 0.7546\n",
            "Epoch 10/10\n",
            "233/233 [==============================] - 40s 174ms/step - loss: 0.0164 - accuracy: 0.9948 - val_loss: 1.9287 - val_accuracy: 0.7487\n",
            "59/59 [==============================] - 3s 51ms/step\n",
            "Accuracy: 0.748657357679914\n",
            "Precision: 0.7551020408163265\n",
            "Recall: 0.7254901960784313\n",
            "F1-score: 0.74\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Conv1D, GlobalMaxPooling1D, Embedding, Bidirectional, LSTM\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Load the dataset\n",
        "train_df = pd.read_csv('Train_data.csv')\n",
        "test_df = pd.read_csv('Test_data.csv')\n",
        "\n",
        "# Split into features and labels\n",
        "X_train = train_df['clean_text']\n",
        "y_train = train_df['IsHatespeech']\n",
        "X_test = test_df['clean_text']\n",
        "y_test = test_df['IsHatespeech']\n",
        "\n",
        "# Tokenize the text\n",
        "max_words = 10000  # Increase the max words to capture more vocabulary\n",
        "max_len = 128\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
        "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "X_train_pad = pad_sequences(X_train_seq, maxlen=max_len)\n",
        "X_test_pad = pad_sequences(X_test_seq, maxlen=max_len)\n",
        "\n",
        "# Load GloVe embeddings\n",
        "embedding_dim = 100\n",
        "embedding_index = {}\n",
        "\n",
        "glove_file = 'glove.6B.100d.txt'\n",
        "\n",
        "with open(glove_file, encoding=\"utf8\") as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embedding_index[word] = coefs\n",
        "\n",
        "# Prepare embedding matrix\n",
        "word_index = tokenizer.word_index\n",
        "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
        "\n",
        "for word, i in word_index.items():\n",
        "    if i < max_words:\n",
        "        embedding_vector = embedding_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "\n",
        "# Define the improved model with CNN and Bidirectional LSTM\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_words, embedding_dim, input_length=max_len, weights=[embedding_matrix], trainable=True))\n",
        "model.add(Conv1D(128, 5, activation='relu'))\n",
        "model.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
        "model.add(GlobalMaxPooling1D())\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train_pad, y_train, epochs=10, batch_size=32, validation_data=(X_test_pad, y_test))\n",
        "\n",
        "# Predict on test data\n",
        "y_pred_prob = model.predict(X_test_pad)\n",
        "y_pred = (y_pred_prob > 0.5).astype(int)  # Convert probabilities to binary predictions\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1-score:\", f1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JwqFw_xOMaFQ",
        "outputId": "e4b81bd2-9162-4735-d7c7-d396d7588ea9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "233/233 [==============================] - 85s 346ms/step - loss: 0.6235 - accuracy: 0.6410 - val_loss: 0.5666 - val_accuracy: 0.7030\n",
            "Epoch 2/10\n",
            "233/233 [==============================] - 71s 303ms/step - loss: 0.5366 - accuracy: 0.7288 - val_loss: 0.5460 - val_accuracy: 0.7288\n",
            "Epoch 3/10\n",
            "233/233 [==============================] - 76s 328ms/step - loss: 0.5050 - accuracy: 0.7565 - val_loss: 0.5248 - val_accuracy: 0.7368\n",
            "Epoch 4/10\n",
            "233/233 [==============================] - 77s 329ms/step - loss: 0.4762 - accuracy: 0.7737 - val_loss: 0.5330 - val_accuracy: 0.7309\n",
            "Epoch 5/10\n",
            "233/233 [==============================] - 69s 297ms/step - loss: 0.4393 - accuracy: 0.7981 - val_loss: 0.5071 - val_accuracy: 0.7524\n",
            "Epoch 6/10\n",
            "233/233 [==============================] - 72s 311ms/step - loss: 0.4113 - accuracy: 0.8153 - val_loss: 0.5145 - val_accuracy: 0.7519\n",
            "Epoch 7/10\n",
            "233/233 [==============================] - 76s 325ms/step - loss: 0.3803 - accuracy: 0.8345 - val_loss: 0.5225 - val_accuracy: 0.7513\n",
            "Epoch 8/10\n",
            "233/233 [==============================] - 71s 307ms/step - loss: 0.3481 - accuracy: 0.8552 - val_loss: 0.5158 - val_accuracy: 0.7642\n",
            "Epoch 9/10\n",
            "233/233 [==============================] - 80s 345ms/step - loss: 0.3138 - accuracy: 0.8713 - val_loss: 0.5292 - val_accuracy: 0.7626\n",
            "Epoch 10/10\n",
            "233/233 [==============================] - 72s 309ms/step - loss: 0.2818 - accuracy: 0.8856 - val_loss: 0.5364 - val_accuracy: 0.7605\n",
            "59/59 [==============================] - 8s 98ms/step\n",
            "Accuracy: 0.7604726100966702\n",
            "Precision: 0.7341269841269841\n",
            "Recall: 0.8061002178649237\n",
            "F1-score: 0.7684319833852543\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Load the dataset\n",
        "train_df = pd.read_csv('Train_data.csv')\n",
        "test_df = pd.read_csv('Test_data.csv')\n",
        "\n",
        "# Split into features and labels\n",
        "X_train = train_df['clean_text']\n",
        "y_train = train_df['IsHatespeech']\n",
        "X_test = test_df['clean_text']\n",
        "y_test = test_df['IsHatespeech']\n",
        "\n",
        "# Tokenize the text\n",
        "max_words = 10000  # Increase the max words to capture more vocabulary\n",
        "max_len = 128\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
        "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "X_train_pad = pad_sequences(X_train_seq, maxlen=max_len)\n",
        "X_test_pad = pad_sequences(X_test_seq, maxlen=max_len)\n",
        "\n",
        "# Load GloVe embeddings\n",
        "embedding_dim = 100\n",
        "embedding_index = {}\n",
        "\n",
        "glove_file = 'glove.6B.100d.txt'\n",
        "\n",
        "with open(glove_file, encoding=\"utf8\") as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embedding_index[word] = coefs\n",
        "\n",
        "# Prepare embedding matrix\n",
        "word_index = tokenizer.word_index\n",
        "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
        "\n",
        "for word, i in word_index.items():\n",
        "    if i < max_words:\n",
        "        embedding_vector = embedding_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "\n",
        "# Define the RNN model with GloVe embeddings\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_words, embedding_dim, input_length=max_len, weights=[embedding_matrix], trainable=True))\n",
        "model.add(Bidirectional(LSTM(128, return_sequences=True)))\n",
        "model.add(Bidirectional(LSTM(64)))\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train_pad, y_train, epochs=10, batch_size=32, validation_data=(X_test_pad, y_test))\n",
        "\n",
        "# Predict on test data\n",
        "y_pred_prob = model.predict(X_test_pad)\n",
        "y_pred = (y_pred_prob > 0.5).astype(int)  # Convert probabilities to binary predictions\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1-score:\", f1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DRMbi0qpO5T5",
        "outputId": "27845880-54f5-42ca-b797-27eabe85bdcf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "233/233 [==============================] - 280s 1s/step - loss: 0.6015 - accuracy: 0.6680 - val_loss: 0.5449 - val_accuracy: 0.7277\n",
            "Epoch 2/10\n",
            "233/233 [==============================] - 268s 1s/step - loss: 0.5137 - accuracy: 0.7471 - val_loss: 0.5256 - val_accuracy: 0.7331\n",
            "Epoch 3/10\n",
            "233/233 [==============================] - 265s 1s/step - loss: 0.4854 - accuracy: 0.7659 - val_loss: 0.5068 - val_accuracy: 0.7438\n",
            "Epoch 4/10\n",
            "233/233 [==============================] - 255s 1s/step - loss: 0.4589 - accuracy: 0.7859 - val_loss: 0.4976 - val_accuracy: 0.7551\n",
            "Epoch 5/10\n",
            "233/233 [==============================] - 260s 1s/step - loss: 0.4334 - accuracy: 0.7944 - val_loss: 0.4946 - val_accuracy: 0.7589\n",
            "Epoch 6/10\n",
            "233/233 [==============================] - 258s 1s/step - loss: 0.4075 - accuracy: 0.8140 - val_loss: 0.4987 - val_accuracy: 0.7583\n",
            "Epoch 7/10\n",
            "233/233 [==============================] - 287s 1s/step - loss: 0.3873 - accuracy: 0.8286 - val_loss: 0.5192 - val_accuracy: 0.7573\n",
            "Epoch 8/10\n",
            "233/233 [==============================] - 262s 1s/step - loss: 0.3650 - accuracy: 0.8431 - val_loss: 0.5090 - val_accuracy: 0.7648\n",
            "Epoch 9/10\n",
            "233/233 [==============================] - 264s 1s/step - loss: 0.3451 - accuracy: 0.8517 - val_loss: 0.5173 - val_accuracy: 0.7696\n",
            "Epoch 10/10\n",
            "233/233 [==============================] - 262s 1s/step - loss: 0.3289 - accuracy: 0.8595 - val_loss: 0.5037 - val_accuracy: 0.7680\n",
            "59/59 [==============================] - 24s 376ms/step\n",
            "Accuracy: 0.7679914070891515\n",
            "Precision: 0.7271028037383177\n",
            "Recall: 0.8474945533769063\n",
            "F1-score: 0.7826961770623743\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Embedding, GRU, Bidirectional, SpatialDropout1D\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Load the dataset\n",
        "train_df = pd.read_csv('Train_data.csv')\n",
        "test_df = pd.read_csv('Test_data.csv')\n",
        "\n",
        "# Split into features and labels\n",
        "X_train = train_df['clean_text'].tolist()\n",
        "y_train = train_df['IsHatespeech'].tolist()\n",
        "X_test = test_df['clean_text'].tolist()\n",
        "y_test = test_df['IsHatespeech'].tolist()\n",
        "\n",
        "# Tokenize the text\n",
        "max_words = 20000  # Increase the max words to capture more vocabulary\n",
        "max_len = 128\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
        "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "X_train_pad = pad_sequences(X_train_seq, maxlen=max_len)\n",
        "X_test_pad = pad_sequences(X_test_seq, maxlen=max_len)\n",
        "\n",
        "# Load GloVe embeddings\n",
        "embedding_dim = 100\n",
        "embedding_index = {}\n",
        "\n",
        "glove_file = 'glove.6B.100d.txt'\n",
        "\n",
        "with open(glove_file, encoding=\"utf8\") as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embedding_index[word] = coefs\n",
        "\n",
        "# Prepare embedding matrix\n",
        "word_index = tokenizer.word_index\n",
        "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
        "\n",
        "for word, i in word_index.items():\n",
        "    if i < max_words:\n",
        "        embedding_vector = embedding_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "\n",
        "# Define the improved RNN model with GloVe embeddings\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_words, embedding_dim, input_length=max_len, weights=[embedding_matrix], trainable=False))\n",
        "model.add(SpatialDropout1D(0.2))\n",
        "model.add(Bidirectional(GRU(128, return_sequences=True)))\n",
        "model.add(SpatialDropout1D(0.2))\n",
        "model.add(Bidirectional(GRU(64)))\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\n",
        "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Learning rate scheduler\n",
        "lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=1)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train_pad, np.array(y_train), epochs=10, batch_size=32,\n",
        "                    validation_data=(X_test_pad, np.array(y_test)),\n",
        "                    callbacks=[lr_scheduler]) # Convert y_train and y_test to NumPy arrays\n",
        "# Predict on test data\n",
        "y_pred_prob = model.predict(X_test_pad)\n",
        "y_pred = (y_pred_prob > 0.5).astype(int)  # Convert probabilities to binary predictions\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1-score:\", f1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yLbsQPsNjh5i",
        "outputId": "e325d8d2-85df-4038-b5a3-8176d9c5ee81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "233/233 [==============================] - 178s 725ms/step - loss: 0.6846 - accuracy: 0.5473 - val_loss: 0.6704 - val_accuracy: 0.6208 - lr: 2.0000e-05\n",
            "Epoch 2/10\n",
            "233/233 [==============================] - 166s 715ms/step - loss: 0.6599 - accuracy: 0.6259 - val_loss: 0.6409 - val_accuracy: 0.6622 - lr: 2.0000e-05\n",
            "Epoch 3/10\n",
            "233/233 [==============================] - 162s 698ms/step - loss: 0.6287 - accuracy: 0.6642 - val_loss: 0.6092 - val_accuracy: 0.6901 - lr: 2.0000e-05\n",
            "Epoch 4/10\n",
            "233/233 [==============================] - 159s 682ms/step - loss: 0.6022 - accuracy: 0.6852 - val_loss: 0.5861 - val_accuracy: 0.6928 - lr: 2.0000e-05\n",
            "Epoch 5/10\n",
            "233/233 [==============================] - 171s 732ms/step - loss: 0.5802 - accuracy: 0.7015 - val_loss: 0.5743 - val_accuracy: 0.7019 - lr: 2.0000e-05\n",
            "Epoch 6/10\n",
            "233/233 [==============================] - 164s 704ms/step - loss: 0.5676 - accuracy: 0.7103 - val_loss: 0.5673 - val_accuracy: 0.7084 - lr: 2.0000e-05\n",
            "Epoch 7/10\n",
            "233/233 [==============================] - 157s 671ms/step - loss: 0.5644 - accuracy: 0.7072 - val_loss: 0.5633 - val_accuracy: 0.7137 - lr: 2.0000e-05\n",
            "Epoch 8/10\n",
            "233/233 [==============================] - 157s 672ms/step - loss: 0.5577 - accuracy: 0.7109 - val_loss: 0.5576 - val_accuracy: 0.7170 - lr: 2.0000e-05\n",
            "Epoch 9/10\n",
            "233/233 [==============================] - 166s 712ms/step - loss: 0.5501 - accuracy: 0.7186 - val_loss: 0.5546 - val_accuracy: 0.7180 - lr: 2.0000e-05\n",
            "Epoch 10/10\n",
            "233/233 [==============================] - 160s 689ms/step - loss: 0.5517 - accuracy: 0.7140 - val_loss: 0.5504 - val_accuracy: 0.7207 - lr: 2.0000e-05\n",
            "59/59 [==============================] - 7s 99ms/step\n",
            "Accuracy: 0.7207303974221267\n",
            "Precision: 0.6870300751879699\n",
            "Recall: 0.7962962962962963\n",
            "F1-score: 0.7376387487386478\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional, Conv1D, GlobalMaxPooling1D, SpatialDropout1D\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Load the dataset\n",
        "train_df = pd.read_csv('Train_data.csv')\n",
        "test_df = pd.read_csv('Test_data.csv')\n",
        "\n",
        "# Split into features and labels\n",
        "X_train = train_df['clean_text'].tolist()\n",
        "y_train = train_df['IsHatespeech'].tolist()\n",
        "X_test = test_df['clean_text'].tolist()\n",
        "y_test = test_df['IsHatespeech'].tolist()\n",
        "\n",
        "# Tokenize the text\n",
        "max_words = 20000  # Increase the max words to capture more vocabulary\n",
        "max_len = 128\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
        "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "X_train_pad = pad_sequences(X_train_seq, maxlen=max_len)\n",
        "X_test_pad = pad_sequences(X_test_seq, maxlen=max_len)\n",
        "\n",
        "# Load GloVe embeddings\n",
        "embedding_dim = 100\n",
        "embedding_index = {}\n",
        "\n",
        "glove_file = 'glove.6B.100d.txt'\n",
        "\n",
        "with open(glove_file, encoding=\"utf8\") as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embedding_index[word] = coefs\n",
        "\n",
        "# Prepare embedding matrix\n",
        "word_index = tokenizer.word_index\n",
        "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
        "\n",
        "for word, i in word_index.items():\n",
        "    if i < max_words:\n",
        "        embedding_vector = embedding_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "\n",
        "# Define the hybrid CNN + BiLSTM model with GloVe embeddings\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_words, embedding_dim, input_length=max_len, weights=[embedding_matrix], trainable=False))\n",
        "model.add(SpatialDropout1D(0.2))\n",
        "model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))\n",
        "model.add(Bidirectional(LSTM(128, return_sequences=True)))\n",
        "model.add(Bidirectional(LSTM(64)))\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# Compile the model\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\n",
        "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Learning rate scheduler\n",
        "lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=1)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train_pad, np.array(y_train), epochs=10, batch_size=32,\n",
        "                    validation_data=(X_test_pad, np.array(y_test)),\n",
        "                    callbacks=[lr_scheduler]) # Convert y_train and y_test to NumPy arrays\n",
        "\n",
        "# Predict on test data\n",
        "y_pred_prob = model.predict(X_test_pad)\n",
        "y_pred = (y_pred_prob > 0.5).astype(int)  # Convert probabilities to binary predictions\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1-score:\", f1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZImLw4NjlCJ4",
        "outputId": "04e6014b-b60d-4d45-dbff-6c3d1a179103"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "233/233 [==============================] - 291s 1s/step - loss: 0.6843 - accuracy: 0.5690 - val_loss: 0.6650 - val_accuracy: 0.6305 - lr: 2.0000e-05\n",
            "Epoch 2/10\n",
            "233/233 [==============================] - 273s 1s/step - loss: 0.6165 - accuracy: 0.6807 - val_loss: 0.5810 - val_accuracy: 0.6987 - lr: 2.0000e-05\n",
            "Epoch 3/10\n",
            "233/233 [==============================] - 278s 1s/step - loss: 0.5682 - accuracy: 0.7079 - val_loss: 0.5652 - val_accuracy: 0.7121 - lr: 2.0000e-05\n",
            "Epoch 4/10\n",
            "233/233 [==============================] - 273s 1s/step - loss: 0.5624 - accuracy: 0.7104 - val_loss: 0.5598 - val_accuracy: 0.7202 - lr: 2.0000e-05\n",
            "Epoch 5/10\n",
            "233/233 [==============================] - 295s 1s/step - loss: 0.5546 - accuracy: 0.7214 - val_loss: 0.5538 - val_accuracy: 0.7170 - lr: 2.0000e-05\n",
            "Epoch 6/10\n",
            "233/233 [==============================] - 278s 1s/step - loss: 0.5463 - accuracy: 0.7244 - val_loss: 0.5520 - val_accuracy: 0.7250 - lr: 2.0000e-05\n",
            "Epoch 7/10\n",
            "233/233 [==============================] - 276s 1s/step - loss: 0.5396 - accuracy: 0.7312 - val_loss: 0.5506 - val_accuracy: 0.7272 - lr: 2.0000e-05\n",
            "Epoch 8/10\n",
            "233/233 [==============================] - 276s 1s/step - loss: 0.5420 - accuracy: 0.7265 - val_loss: 0.5474 - val_accuracy: 0.7282 - lr: 2.0000e-05\n",
            "Epoch 9/10\n",
            "233/233 [==============================] - 275s 1s/step - loss: 0.5325 - accuracy: 0.7336 - val_loss: 0.5420 - val_accuracy: 0.7288 - lr: 2.0000e-05\n",
            "Epoch 10/10\n",
            "233/233 [==============================] - 278s 1s/step - loss: 0.5301 - accuracy: 0.7328 - val_loss: 0.5394 - val_accuracy: 0.7331 - lr: 2.0000e-05\n",
            "59/59 [==============================] - 19s 301ms/step\n",
            "Accuracy: 0.7330827067669173\n",
            "Precision: 0.6925892040256175\n",
            "Recall: 0.8246187363834423\n",
            "F1-score: 0.7528592739930382\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aNvr83LOsacN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}