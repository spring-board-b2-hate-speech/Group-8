{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad477712",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9901609b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\shrey\\\\Downloads'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8965dad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Preprocessed_cleaned_Final_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ca9bdc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9310 entries, 0 to 9309\n",
      "Data columns (total 5 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   Text          9310 non-null   object\n",
      " 1   IsHatespeech  9310 non-null   int64 \n",
      " 2   tokens        9310 non-null   object\n",
      " 3   clean_text    9307 non-null   object\n",
      " 4   text_length   9310 non-null   int64 \n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 363.8+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8fc3e4",
   "metadata": {},
   "source": [
    "### Word Tokenization\n",
    "#### Word tokenization is the process of splitting a string of text into individual words or tokens. It is a fundamental step in natural language processing (NLP) and text mining tasks. By breaking down text into words, it allows for more detailed and granular analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c36f26aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    ['frustratingly', 'small', 'please', 'find', '...\n",
       "1    ['padding', 'cell', 'padding', 'class', 'mainp...\n",
       "2    ['scenario', 'present', 'see', 'believe', 'peo...\n",
       "3    ['go', 'inside', 'tomorrow', 'hate', 'every', ...\n",
       "4    ['retweet', 'china', 'bird', 'flu', 'outbreak'...\n",
       "Name: tokens, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tokens'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce32e86e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>IsHatespeech</th>\n",
       "      <th>tokens</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>text_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>frustratingly small please find an image at le...</td>\n",
       "      <td>0</td>\n",
       "      <td>['frustratingly', 'small', 'please', 'find', '...</td>\n",
       "      <td>frustratingly small please find image least pi...</td>\n",
       "      <td>117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>padding cell padding class mainpagebg solid ve...</td>\n",
       "      <td>0</td>\n",
       "      <td>['padding', 'cell', 'padding', 'class', 'mainp...</td>\n",
       "      <td>padding cell padding class mainpagebg solid ve...</td>\n",
       "      <td>1192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>from the scenarios you present i see you belie...</td>\n",
       "      <td>0</td>\n",
       "      <td>['scenario', 'present', 'see', 'believe', 'peo...</td>\n",
       "      <td>scenario present see believe people robot abra...</td>\n",
       "      <td>560</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  IsHatespeech  \\\n",
       "0  frustratingly small please find an image at le...             0   \n",
       "1  padding cell padding class mainpagebg solid ve...             0   \n",
       "2  from the scenarios you present i see you belie...             0   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  ['frustratingly', 'small', 'please', 'find', '...   \n",
       "1  ['padding', 'cell', 'padding', 'class', 'mainp...   \n",
       "2  ['scenario', 'present', 'see', 'believe', 'peo...   \n",
       "\n",
       "                                          clean_text  text_length  \n",
       "0  frustratingly small please find image least pi...          117  \n",
       "1  padding cell padding class mainpagebg solid ve...         1192  \n",
       "2  scenario present see believe people robot abra...          560  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae6b569",
   "metadata": {},
   "source": [
    "### Word2Vec Embeddings\n",
    "#### Pros:\n",
    "\n",
    "#### Captures semantic relationships between words.\n",
    "#### Creates dense, low-dimensional representations.\n",
    "#### Effective for deep learning models and tasks requiring word similarity and analogy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "643632ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec Embeddings:\n",
      "                                              clean_text  \\\n",
      "0     frustratingly small please find image least pi...   \n",
      "1     padding cell padding class mainpagebg solid ve...   \n",
      "2     scenario present see believe people robot abra...   \n",
      "3     go inside tomorrow hate every time feel cheap ...   \n",
      "4     retweet china bird flu outbreak good sign bad ...   \n",
      "...                                                 ...   \n",
      "9305  chastity really quality girl woman grow associ...   \n",
      "9306  wow like folk riding subway today woman like m...   \n",
      "9307                                        mean nigger   \n",
      "9308  let know girl run shit round sexist believe sh...   \n",
      "9309  gavin williamson still still work department e...   \n",
      "\n",
      "                                               word2vec  \n",
      "0     [-0.2121953, -0.0012929715, -0.19467257, -0.16...  \n",
      "1     [-0.20271866, 0.021500044, -0.19714814, -0.149...  \n",
      "2     [-0.20705277, 0.03751097, -0.20214587, -0.1281...  \n",
      "3     [-0.2520867, 0.04619578, -0.22891563, -0.15858...  \n",
      "4     [-0.22240885, 0.0825314, -0.19528674, -0.11517...  \n",
      "...                                                 ...  \n",
      "9305  [-0.18652068, 0.03031387, -0.1828261, -0.12482...  \n",
      "9306  [-0.21467373, 0.052689414, -0.1686534, -0.1124...  \n",
      "9307  [-0.26683748, 0.09356297, -0.22939235, -0.1042...  \n",
      "9308  [-0.20755814, 0.060174786, -0.19850041, -0.113...  \n",
      "9309  [-0.21033965, 0.027146902, -0.187323, -0.12781...  \n",
      "\n",
      "[9310 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "word2vec_model = Word2Vec(sentences=df['tokens'], vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "def average_word_vectors(tokens, model, num_features):\n",
    "    feature_vec = np.zeros((num_features,), dtype=\"float32\")\n",
    "    n_words = 0\n",
    "    for word in tokens:\n",
    "        if word in model.wv:\n",
    "            n_words += 1\n",
    "            feature_vec = np.add(feature_vec, model.wv[word])\n",
    "    if n_words > 0:\n",
    "        feature_vec = np.divide(feature_vec, n_words)\n",
    "    return feature_vec\n",
    "\n",
    "df['word2vec'] = df['tokens'].apply(lambda tokens: average_word_vectors(tokens, word2vec_model, 100))\n",
    "\n",
    "print(\"Word2Vec Embeddings:\\n\", df[['clean_text', 'word2vec']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6fe066f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [-0.2121953, -0.0012929715, -0.19467257, -0.16...\n",
       "1       [-0.20271866, 0.021500044, -0.19714814, -0.149...\n",
       "2       [-0.20705277, 0.03751097, -0.20214587, -0.1281...\n",
       "3       [-0.2520867, 0.04619578, -0.22891563, -0.15858...\n",
       "4       [-0.22240885, 0.0825314, -0.19528674, -0.11517...\n",
       "                              ...                        \n",
       "9305    [-0.18652068, 0.03031387, -0.1828261, -0.12482...\n",
       "9306    [-0.21467373, 0.052689414, -0.1686534, -0.1124...\n",
       "9307    [-0.26683748, 0.09356297, -0.22939235, -0.1042...\n",
       "9308    [-0.20755814, 0.060174786, -0.19850041, -0.113...\n",
       "9309    [-0.21033965, 0.027146902, -0.187323, -0.12781...\n",
       "Name: word2vec, Length: 9310, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['word2vec']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "438b4e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9310 entries, 0 to 9309\n",
      "Data columns (total 6 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   Text          9310 non-null   object\n",
      " 1   IsHatespeech  9310 non-null   int64 \n",
      " 2   tokens        9310 non-null   object\n",
      " 3   clean_text    9307 non-null   object\n",
      " 4   text_length   9310 non-null   int64 \n",
      " 5   word2vec      9310 non-null   object\n",
      "dtypes: int64(2), object(4)\n",
      "memory usage: 436.5+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0306371a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
